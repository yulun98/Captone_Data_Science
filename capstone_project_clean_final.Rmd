---
title: "capstone_projec_clean_final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Library Loading
```{r, results='hide'}
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringi)
library(gridExtra)
library(tm)
library(RWeka)
library(partitions)
library(R.utils)
library(parallel)
```

# Data loading
The following code were ran once only to save memory
```{r}
#fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

#download.file(fileUrl, destfile = "C:/Users/yulun/Desktop/coursera R programming/Capstone/Coursera-SwiftKey.zip")

#unzip the data
#unzip("Coursera-SwiftKey.zip")

twitter_file <- file("C:/Users/yulun/Desktop/coursera R programming/Capstone/data/final/en_US/en_US.twitter.txt", "r")
blogs_file <- file("C:/Users/yulun/Desktop/coursera R programming/Capstone/data/final/en_US/en_US.blogs.txt", "r")
news_file <- file("C:/Users/yulun/Desktop/coursera R programming/Capstone/data/final/en_US/en_US.news.txt", "r")

#badwords_fileUrl <-"http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
#download.file(badwords_fileUrl, destfile = "C:/Users/yulun/Desktop/coursera R programming/Capstone/data/bad-words.txt")

badwords_file <- file("C:/Users/yulun/Desktop/coursera R programming/Capstone/data/bad-words.txt")
```

## Data Exploration
The following code were ran once only to save memory
```{r}
twitter_data <- readLines(twitter_file, encoding = "UTF-8", skipNul = TRUE)
close(twitter_file)

blogs_data <- readLines(blogs_file, encoding = "UTF-8", skipNul = TRUE)
close(blogs_file)

news_data <- readLines(news_file, encoding = "UTF-8", skipNul = TRUE)
close(news_file)

badwords_data <- readLines(badwords_file, encoding = "UTF-8", skipNul = TRUE)
close(badwords_file)
```

# Sampling Data
Due to limited computing power of my current laptop, only 10% of each of the files were used for the predictive model building
```{r}
set.seed(1234)
sample_data <- function(file, proportion) {
        sampling <- sample(1:length(file), length(file)*proportion)
        sampled_data <- file[sampling]
        sampled_data
}
sampled_twitter <- sample_data(twitter_data, 0.1)
sampled_blogs <- sample_data(blogs_data, 0.1)
sampled_news <- sample_data(news_data, 0.1)

merged_sample_data <- c(sampled_twitter, sampled_blogs, sampled_news)

writeLines(merged_sample_data, "C:/Users/yulun/Desktop/coursera R programming/Capstone/data/final/en_US/merged_sample_data.txt")

head(merged_sample_data) 

summary(merged_sample_data)
```

```{r}
#Merged
merged_file <- file("C:/Users/yulun/Desktop/coursera R programming/Capstone/data/final/en_US/merged_sample_data.txt")
merged_sample_data <- readLines(merged_file, encoding = "UTF-8", skipNul = TRUE)
```

#Data cleaning
```{r}
#Creating a volatile corpus
vol_corpus <- VCorpus(DirSource("C:/Users/yulun/Desktop/coursera R programming/Capstone/data/final/en_US/", pattern = "merged_sample_data.txt", encoding = "UTF-8"))
```

```{r}
#Data cleaning using tm_map
process_corpus<-function(v) { 
     v<-tm_map(v,removeNumbers)
     v<-tm_map(v,stripWhitespace)
     v<-tm_map(v, removePunctuation,preserve_intra_word_dashes = TRUE)
     v<-tm_map(v, content_transformer(tolower))
     #v<-tm_map(v, removeWords, stopwords("english"))
     v<-tm_map(v, PlainTextDocument)
     v
}

#Function for making n-gram
n.grams<-function(corpus, n){
     token<-function(x){
          RWeka::NGramTokenizer(x, RWeka::Weka_control(min = n, max = n))
     }
     tdm <- TermDocumentMatrix(corpus, control = list(tokenizer = token))
     tdm
}

#Function to extract the N grams and sort
tdm_sort <- function (tdm) {
  tdm_m <- as.matrix(tdm)
  tdm_df <- as.data.frame(tdm_m)
  colnames(tdm_df) <- "Count"
  tdm_df <- tdm_df[order(-tdm_df$Count), , drop = FALSE]
  tdm_df
}

cleaned_data <- process_corpus(vol_corpus)
#Calculating n-gram
ngram_2 <- n.grams(cleaned_data,2)
ngram_3 <- n.grams(cleaned_data,3)
ngram_4 <- n.grams(cleaned_data,4)
#Extract n_gram and sort
ngram_2 <- tdm_sort(ngram_2)
ngram_3 <- tdm_sort(ngram_3)
ngram_4 <- tdm_sort(ngram_4)
```

```{r}
bigram <- data.frame(names=rownames(ngram_2),count=ngram_2$Count)
bigram$names <- as.character(bigram$names)
bigram_split <- strsplit(as.character(bigram$names),split=" ")
bigram <- transform(bigram,
                    first = sapply(bigram_split,"[[",1),
                    second = sapply(bigram_split,"[[",2))
bigram <- data.frame(unigram = bigram$first,
                     bigram = bigram$second,
                     freq = bigram$count,
                     stringsAsFactors=FALSE)

trigram <- data.frame(names=rownames(ngram_3),count=ngram_3$Count)
trigram$names <- as.character(trigram$names)
trigram_split <- strsplit(as.character(trigram$names),split=" ")
trigram <- transform(trigram,
                     first = sapply(trigram_split,"[[",1),
                     second = sapply(trigram_split,"[[",2),
                     third = sapply(trigram_split,"[[",3))
trigram <- data.frame(unigram = trigram$first,
                      bigram = trigram$second, 
                      trigram = trigram$third, 
                      freq = trigram$count,
                      stringsAsFactors=FALSE)

quadgram <- data.frame(names=rownames(ngram_4),count=ngram_4$Count)
quadgram$names <- as.character(quadgram$names)
quadgram_split <- strsplit(as.character(quadgram$names),split=" ")
quadgram <- transform(quadgram,
                      first = sapply(quadgram_split,"[[",1),
                      second = sapply(quadgram_split,"[[",2),
                      third = sapply(quadgram_split,"[[",3), 
                      fourth = sapply(quadgram_split,"[[",4))
quadgram <- data.frame(unigram = quadgram$first,
                       bigram = quadgram$second, 
                       trigram = quadgram$third, 
                       quadgram = quadgram$fourth, 
                       freq = quadgram$count,
                       stringsAsFactors=FALSE)
```

Saving them as RDS
```{r}
write.csv(bigram[bigram$freq > 1,],"C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/bigram.csv",row.names=F)
write.csv(trigram[trigram$freq > 1,],"C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/trigram.csv",row.names=F)
write.csv(quadgram[quadgram$freq > 1,],"C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/quadgram.csv",row.names=F)

bigram <- read.csv("C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/bigram.csv",stringsAsFactors = F)
trigram <- read.csv("C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/trigram.csv",stringsAsFactors = F)
quadgram <- read.csv("C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/quadgram.csv",stringsAsFactors = F)

saveRDS(bigram,"C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/bigram.RData")
saveRDS(trigram,"C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/trigram.RData")
saveRDS(quadgram,"C:/Users/yulun/Desktop/coursera R programming/Capstone/capstone_project_final/quadgram.RData")
```



